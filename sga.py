# -*- coding: utf-8 -*-
"""SGA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cABgd-fYDm82SvFstNezrVSjd0EZlC3v
"""

from google.colab import drive
drive.mount('/content/drive')

import os
for root, dirs, files in os.walk('/content/drive/MyDrive/internship/'):
  for name in dirs:
    print(os.path.join(root, name))
  for name in files:
    print(os.path.join(root, name))

# Commented out IPython magic to ensure Python compatibility.
import sys
sys.path.append('/content/drive/MyDrive/internship/PreprocessNotebook/imports/')
import time
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import os, random
import utils
from utils_vis import plot_comparison, plot_labels_color
from utils import compute_metric_dc
import nibabel as nib
from sklearn.model_selection import KFold

# %matplotlib inline

SHUFFLE_BUFFER = 4000
max_epochs = 20
BATCH_SIZE = 8
lr = 0.001
opt = tf.keras.optimizers.Adam(lr)
ver = 'model_self_attention_03' #save version
dropout=0.2 #dropout rate
hn = 'he_normal' #kernel initializer
tfrecords_read_dir = '/content/drive/MyDrive/internship/archive/tfrecords/LGG'
stack_npy = "/content/drive/MyDrive/internship/archive/imagestack/"

xent = tf.keras.losses.CategoricalCrossentropy()

def generalized_dice(y_true, y_pred, smooth = 1e-5):
    """
    Generalized Dice Score
    https://arxiv.org/pdf/1707.03237
    https://github.com/Mehrdad-Noori/Brain-Tumor-Segmentation/blob/master/loss.py
    """

    y_true    = tf.reshape(y_true,shape=(-1,4))
    y_pred    = tf.reshape(y_pred,shape=(-1,4))
    sum_p     = tf.reduce_sum(y_pred, -2)
    sum_r     = tf.reduce_sum(y_true, -2)
    sum_pr    = tf.reduce_sum(y_true * y_pred, -2)
    weights   = tf.math.pow(tf.math.square(sum_r) + smooth, -1)
    generalized_dice = (2 * tf.reduce_sum(weights * sum_pr)) / (tf.reduce_sum(weights * (sum_r + sum_p)))
    return generalized_dice

def generalized_dice_loss(y_true, y_pred):
    return 1-generalized_dice(y_true, y_pred)

def custom_loss(y_true, y_pred):

    """
    The final loss function consists of the summation of two losses "GDL" and "CE"
    with a regularization term.
    """

    return generalized_dice_loss(y_true, y_pred) + 1.25 * xent(y_true, y_pred)

def data_aug(imgs):
    choice = np.random.randint(0,4)
    #no augmentation
    if choice==0:
        x = imgs
    #flip up and down
    if choice==1:
        x = tf.image.flip_up_down(imgs)
    #flip left and right
    if choice==2:
        x = tf.image.flip_left_right(imgs)
    #rotation based on angle
    if choice==3:
        n_rot = np.random.randint(1,4)
        x = tf.image.rot90(imgs, k=n_rot)
    return x

# template for guided attention block
layer_name_p01 = ['pam01_conv01', 'pam01_conv02', 'pam01_softmax', 'pam01_conv03',
                  'pam01_alpha','pam01_add']
layer_name_c01 = ['cam01_softmax', 'cam01_alpha','cam01_add']
layer_name_p02 = ['pam02_conv01', 'pam02_conv02', 'pam02_softmax', 'pam02_conv03',
                  'pam02_alpha', 'pam02_add']
layer_name_c02 = ['cam02_softmax', 'cam02_alpha','cam02_add']
layer_name_template = [layer_name_p01, layer_name_c01, layer_name_p02, layer_name_c02]

layer_name_ga = []
for b in range(1,4):
    layer_block = []
    for layer in layer_name_template:
        layer_internal = [i+'block0{}'.format(b) for i in layer]
        layer_block.append(layer_internal)
    layer_name_ga.append(layer_block)

!pip install tensorflow-addons

from tensorflow.keras.layers import Conv3D, UpSampling3D, GaussianNoise, Input, PReLU, Add, UpSampling3D, Conv3DTranspose, Concatenate
from tensorflow.keras.models import Model
from tensorflow_addons.layers import GroupNormalization
import tensorflow as tf

hn = 'he_normal'  # kernel initializer

def conv_block_3D(x, filters, norm_fn='gn', kernel_size=3,
                  kernel_initializer=hn, acti_fn='prelu', dropout_rate=None):
    '''
    Dual convolution block with [full pre-activation], Norm -> Acti -> Conv
    :param x: Input features
    :param filters: A list that contains the number of filters for 1st and 2nd convolutional layer
    :param norm_fn: Tensorflow function for normalization, 'bn' for Batch Norm, 'gn' for Group Norm
    :param kernel_size: Kernel size for both convolutional layer with 3x3 as default
    :param kernel_initializer: Initializer for kernel weights with 'glorot uniform' as default
    :param acti_fn: Tensorflow function for activation, 'relu' for ReLU, 'prelu' for PReLU
    :param dropout_rate: Specify dropouts for layers
    :return: Feature maps of same size as input with number of filters equivalent to the last layer
    '''
    assert type(filters) == list, "Please input filters of type list."
    assert acti_fn != None, 'There should be an activation function specified'
    # 1st convolutional block
    if norm_fn == 'bn':
        x = tf.keras.layers.BatchNormalization()(x)
    elif norm_fn == 'gn':
        x = GroupNormalization(groups=4)(x)  # Adjusted GroupNormalization with groups=4
    if acti_fn == 'relu':
        x = tf.keras.layers.ReLU()(x)
    elif acti_fn == 'prelu':
        x = PReLU(shared_axes=[1, 2, 3])(x)
    if dropout_rate != None:
        x = tf.keras.layers.Dropout(dropout_rate)(x)
    x = Conv3D(filters[0], kernel_size, padding='same', kernel_initializer=kernel_initializer)(x)
    # 2nd convolutional block
    if norm_fn == 'bn':
        x = tf.keras.layers.BatchNormalization()(x)
    elif norm_fn == 'gn':
        x = GroupNormalization(groups=4)(x)  # Adjusted GroupNormalization with groups=4
    if acti_fn == 'relu':
        x = tf.keras.layers.ReLU()(x)
    elif acti_fn == 'prelu':
        x = PReLU(shared_axes=[1, 2, 3])(x)
    x = Conv3D(filters[1], kernel_size, padding='same', kernel_initializer=kernel_initializer)(x)
    return x


def down_sampling_3D(x, filters, norm_fn='gn', kernel_size=3, acti_fn='relu',
                     kernel_initializer=hn, dropout_rate=None):
    '''
    Down sampling function version 2 with Convolutional layer of stride 2 as downsampling operation, with
    [full pre-activation], Norm -> Acti -> Conv
    :param x: Input features
    :param filters: Number of filters for Convolutional layer of stride 2
    :param norm_fn: Tensorflow function for normalization, 'bn' for Batch Norm, 'gn' for Group Norm
    :param kernel_size: Kernel size for both convolutional layer with 3x3 as default
    :param acti_fn: Tensorflow function for activation, 'relu' for ReLU, 'prelu' for PReLU
    :param kernel_initializer: Initializer for kernel weights with 'glorot uniform' as default
    :param dropout_rate: Specify dropouts for layers
    :return: Feature maps of size scaled down by 2 with number of filters specified
    '''
    assert acti_fn != None, 'There should be an activation function specified'
    # normalization
    if norm_fn == 'bn':
        x = tf.keras.layers.BatchNormalization()(x)
    elif norm_fn == 'gn':
        x = GroupNormalization(groups=4)(x)  # Adjusted GroupNormalization with groups=4
    if acti_fn == 'relu':
        x = tf.keras.layers.ReLU()(x)
    # activation
    elif acti_fn == 'prelu':
        x = PReLU(shared_axes=[1, 2, 3])(x)
    if dropout_rate != None:
        x = tf.keras.layers.Dropout(dropout_rate)(x)
    # normal mode
    x = Conv3D(filters, kernel_size, strides=(2, 2, 2), padding='same', kernel_initializer=kernel_initializer)(x)
    return x


def res_block_3D(x_in, filters, norm_fn='gn', kernel_size=3,
                 kernel_initializer=hn, acti_fn='prelu', dropout_rate=None):
    '''
    This function constructs the residual block in 3D by input->conv_block_3D->concat([input,conv_output])
    :param x_in: Input features
    :param filters: A list that contains the number of filters for 1st and 2nd convolutional layer
    :param norm_fn: Tensorflow function for normalization, 'bn' for Batch Norm, 'gn' for Group Norm
    :param kernel_size: Kernel size for both convolutional layer with 3x3 as default
    :param kernel_initializer: Initializer for kernel weights with 'glorot uniform' as default
    :param acti_fn: Tensorflow function for activation, 'relu' for ReLU, 'prelu' for PReLU
    :param dropout_rate: Specify dropouts for layers
    :return: Residual block output => concatenating input with 2*convolutional output
    '''
    assert len(filters) == 2, "Please assure that there are 2 values for filters."
    output_conv_block = conv_block_3D(x_in, filters, norm_fn=norm_fn, kernel_size=kernel_size,
                                      kernel_initializer=kernel_initializer, acti_fn=acti_fn, dropout_rate=dropout_rate)
    output_add = Add()([output_conv_block, x_in])
    return output_add


def up_3D(x_in, filters, merge, kernel_initializer=hn, size=(2, 2, 2)):
    '''
    This function carries out the operation of deconvolution => upsampling + convolution, and
    concatenating feature maps from the skip connection with the deconv feature maps
    @param x_in: input feature
    @param filters: Number of filters
    @param merge: feature maps from the skip connection
    @param kernel_initializer: Initializer for kernel weights with 'glorot uniform' as default
    @param size: Upsampling size, by default (1,2,2)
    @return: concatenated feature maps of skip connection output and upsampled feature maps from previous output
    '''
    u = UpSampling3D(size)(x_in)
    conv = Conv3D(filters=filters, kernel_size=3, padding='same',
                  kernel_initializer=kernel_initializer)(u)
    conv = PReLU(shared_axes=[1, 2, 3])(conv)
    concat = tf.concat([merge, conv], axis=-1)
    return concat


def attention_block_3D(input_signal, gated_signal, filters, kernel_initializer=hn):
    # input signal feature maps
    is_fm = Conv3D(filters, kernel_size=1, strides=2, padding='same',
                   kernel_initializer=kernel_initializer)(input_signal)
    # gated signal feature maps
    gs_fm = Conv3D(filters, kernel_size=1, strides=1, padding='same',
                   kernel_initializer=kernel_initializer)(gated_signal)

    # Ensure the shapes match after convolutions
    if is_fm.shape != gs_fm.shape:
        gs_fm = Conv3D(filters, kernel_size=1, strides=2, padding='same',
                       kernel_initializer=kernel_initializer)(gated_signal)

    # element wise sum
    add = Add()([is_fm, gs_fm])
    acti = tf.keras.layers.ReLU()(add)

    # downsampled attention coefficient
    bottle_neck = Conv3D(1, kernel_size=1, activation='sigmoid',
                         kernel_initializer=kernel_initializer)(acti)

    # bilinear interpolation to get attention coefficient
    alpha = UpSampling3D(size=2)(bottle_neck)

    # filter off input signal's features with attention coefficient
    multi = tf.keras.layers.Multiply()([input_signal, alpha])

    return multi



def DeepAttVnet(x):
    # filter list
    f_list = [8, 16, 32, 64]
    # inject gaussian noise
    gauss1 = GaussianNoise(0.01)(x)
    # -----------down sampling path--------------------------------------
    # 1st block [128, 200, 200, 4]
    conv_01 = Conv3D(f_list[0], 3, padding='same', kernel_initializer=hn)(gauss1)
    conv_01 = PReLU(shared_axes=[1, 2, 3])(conv_01)
    res_block01 = res_block_3D(conv_01, filters=[f_list[0] * 2, f_list[0]])
    # 2nd block [128, 100, 100, 4]
    down_01 = down_sampling_3D(res_block01, filters=f_list[1])
    res_block02 = res_block_3D(down_01, filters=[f_list[1] * 2, f_list[1]])
    # 3rd block [128, 50, 50, 4]
    down_02 = down_sampling_3D(res_block02, filters=f_list[2])
    res_block03 = res_block_3D(down_02, filters=[f_list[2] * 2, f_list[2]])
    # 4th block [128, 25, 25, 4] *latent space
    down_03 = down_sampling_3D(res_block03, filters=f_list[3])
    res_block04 = res_block_3D(down_03, filters=[f_list[3] * 2, f_list[3]])

    # -----------up sampling path-----------------------------------------
    # 1st attention block---
    att_01 = attention_block_3D(res_block03, res_block04, f_list[2])
    # 1st up [128, 50, 50, 4]
    up_01 = up_3D(res_block04, f_list[2], att_01)
    up_conv01 = conv_block_3D(up_01, filters=[f_list[2], f_list[2]])
    # 1st block segmentation output
    seg_01 = Conv3D(4, kernel_size=1, padding='same', kernel_initializer=hn)(up_conv01)

    # 2nd attention block---
    att_02 = attention_block_3D(res_block02, up_conv01, f_list[1])
    # 2nd up [128, 100, 100, 4]
    up_02 = up_3D(up_conv01, f_list[1], att_02)
    up_conv02 = conv_block_3D(up_02, filters=[f_list[1], f_list[1]])
    # 2nd block segmentation output
    seg_02 = Conv3D(4, kernel_size=1, padding='same', kernel_initializer=hn)(up_conv02)

    # 3rd attention block---
    att_03 = attention_block_3D(res_block01, up_conv02, f_list[0])
    # 3rd up [128, 200, 200, 4]
    up_03 = up_3D(up_conv02, f_list[0], att_03)
    up_conv03 = conv_block_3D(up_03, filters=[f_list[0], f_list[0]])
    # 3rd block segmentation output
    seg_03 = Conv3D(4, kernel_size=1, padding='same', kernel_initializer=hn)(up_conv03)

    # Deep Supervision---
    # Add all segmentation output before inputting into the output layer
    # upsample to fit the size of seg_02
    seg_01_up = UpSampling3D(size=2)(seg_01)
    add_0102 = Add()([seg_01_up, seg_02])
    # upsample to fit the size of seg_03
    add_0102_up = UpSampling3D(size=2)(add_0102)
    add_010203 = Add()([add_0102_up, seg_03])

    # segmentation output
    output = Conv3D(4, kernel_size=1, activation='softmax',
                    kernel_initializer=hn)(add_010203)
    return output

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, UpSampling2D
from tensorflow.keras.models import Model

# forward fonksiyonunun tanımı
def forward(input_tensor):
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_tensor)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = Conv2DTranspose(16, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)

    outputs = Conv2D(4, (1, 1), activation='softmax', padding='same')(x)
    return outputs

# Modelin oluşturulması
input_layer = Input(shape=(200, 200, 4))
output_layer = forward(input_layer)
model = Model(input_layer, output_layer)

# Modelin derlenmesi
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

xent_logit = tf.keras.losses.CategoricalCrossentropy(from_logits=True)

@tf.function
def train_fn(image, label):
    with tf.GradientTape() as tape:
        output_xent, output_dice,_,_ = model(image, training=True)
        loss_dice = generalized_dice_loss(label, output_dice)
        loss_xents=[]
        for seg in output_xent:
            loss_xent = xent_logit(label, seg)
            loss_xents.append(loss_xent)
        loss_total = sum(loss_xents)+loss_dice
    gradients = tape.gradient(loss_total, model.trainable_variables)
    opt.apply_gradients(zip(gradients, model.trainable_variables))

    return output_dice, loss_total, gradients

@tf.function
def val_fn(image, label):
    model_output = model(image, training=False)
    loss = custom_loss(label, model_output)
    return model_output, loss

    import tensorflow as tf

@tf.function
def train_fn(image, label):
    with tf.GradientTape() as tape:
        img_seg = model(image, training=True)
        loss = tf.keras.losses.categorical_crossentropy(label, img_seg)
        loss = tf.reduce_mean(loss)
    gradients = tape.gradient(loss, model.trainable_variables)
    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return img_seg, loss, gradients

import os
import time
import numpy as np
import random
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, UpSampling2D
from tensorflow.keras.models import Model

# Model tanımı (forward fonksiyonu)
def forward(input_tensor):
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_tensor)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = Conv2DTranspose(16, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)

    outputs = Conv2D(4, (1, 1), activation='softmax', padding='same')(x)
    return outputs

# Modelin oluşturulması
input_layer = Input(shape=(200, 200, 4))
output_layer = forward(input_layer)
model = Model(input_layer, output_layer)

# Modelin derlenmesi
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Eğitim işlevi
@tf.function
def train_fn(image, label):
    with tf.GradientTape() as tape:
        img_seg = model(image, training=True)
        loss = tf.keras.losses.categorical_crossentropy(label, img_seg)
        loss = tf.reduce_mean(loss)
    gradients = tape.gradient(loss, model.trainable_variables)
    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return img_seg, loss, gradients

# Eğitim ayarları
BATCH_SIZE = 2  # Küçük bir batch boyutu kullanın
max_epochs = 2  # Eğitim için maksimum epoch sayısı

# Eğitim döngüsü
epochs = 1
loss_list = []
start_runtime = time.time()
while epochs <= max_epochs:
    start = time.time()
    print()
    print("Epochs {:2d}".format(epochs))
    steps = 1
    ds = os.listdir(tfrecords_read_dir)
    random.shuffle(ds)
    loss_inner = []
    for tf_re in ds:
        tf_dir = os.path.join(tfrecords_read_dir, tf_re)
        dataset = utils.parse_tfrecord(tf_dir).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE)
        for imgs in dataset:
            imgs = data_aug(imgs)
            image = imgs[:, 20:220, 20:220, :4]
            label = imgs[:, 20:220, 20:220, -1]
            label = tf.where(label == 4, 3, label)
            label = tf.keras.utils.to_categorical(label, num_classes=4)
            img_seg, loss, gradients = train_fn(image, label)
            img_seg = tf.math.argmax(img_seg, -1, output_type=tf.int32)
            label = tf.math.argmax(label, -1, output_type=tf.int32)
            acc = tf.reduce_mean(tf.cast(tf.equal(img_seg, label), tf.float32))
            loss_inner.append(loss.numpy())
            if epochs % 5 == 0:
                model.save_weights('/content/models.h5')
            if steps % 500 == 0:
                input_img = [image[0, :, :, 0], plot_labels_color(label[0]), plot_labels_color(img_seg[0])]
                caption = ['Input Image', 'Ground Truth', 'Model Output']
                plot_comparison(input_img, caption, n_col=3, figsize=(10, 10))
                acc_stp = tf.reduce_mean(tf.cast(tf.equal(img_seg[0], label[0]), tf.float32))
                dc_list_stp = compute_metric_dc(label[0], img_seg[0])
                print("Steps: {}, Loss:{}".format(steps, loss.numpy()))
                print("Accurary: {}".format(acc_stp))
                print("Seq: TC, ET, WT")
                print("Dice coefficient: {}".format(dc_list_stp))
                print("Gradient min:{}, max:{}".format(np.min(gradients[0]), np.max(gradients[0])))
            steps += 1
    loss_list.append(np.mean(loss_inner))
    elapsed_time = (time.time() - start) / 60
    print("Compute time per epochs: {:.2f} mins".format(elapsed_time))
    epochs += 1

elapsed_time_runtime = (time.time() - start_runtime) / 60
print()
print('----------------------------------<END>---------------------------------')
print("Total run time for {} epochs: {:.2f} mins".format(epochs, elapsed_time_runtime))

model.save_weights('/content/models.h5'.format(ver))

model.load_weights('/content/models.h5'.format(ver))
def output_fn(image):
    b,w,h,c = image.shape
    model.trainable = False
    _, model_output,_,_ = model(image)
    # we need [240,240,155] to input into cloud validation
    if w!=240:
        #padding constant
        p = int(240-w)//2
        padding = tf.constant([[0,0],[p,p],[p,p],[0,0]]) #p=20
        model_output = tf.pad(model_output, padding, "CONSTANT")
    return model_output

import os
import numpy as np
import nibabel as nib
import tensorflow as tf

ds = '/content/drive/MyDrive/internship/archive/BraTS2020_preprocessed/Training_pre/LGG/'
save_path = '/content/submission/'
actual_label = '/content/drive/MyDrive/internship/archive/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_seg.nii'
brain_affine = nib.load(actual_label).affine

# Modelinizi tanımlayın
# Bu örnekte modelinizin hazır olduğunu varsayıyorum
# model = ... (Modelinizi buraya tanımlayın)

def output_fn(image):
    # Modeli değerlendirme moduna getirin
    model.trainable = False
    # Modeli çalıştırın ve tek bir çıkış alın
    output = model(image)
    return output

# Sadece dosyaları işleme
for file in sorted(os.listdir(ds)):
    file_path = os.path.join(ds, file)

    if os.path.isfile(file_path):  # Eğer bir dosya ise
        patient_id = file.split('.')[0]
        imgs = np.load(file_path)
        image = imgs[:, 20:220, 20:220, :4]
        seg_output = np.zeros((200, 200, 155))
        for i in range(image.shape[0]):
            inp = tf.expand_dims(image[i], 0)
            img_seg = output_fn(inp)  # Modeli çalıştırma fonksiyonu
            # Model çıkışını numpy array'e dönüştürme ve argmax işlemi
            img_seg = img_seg.numpy()  # TensorFlow tensor'ı numpy array'e dönüştür
            seg_output[:, :, i] = np.argmax(img_seg, -1)

        # Convert label from 4 to 3 and np array and cast as int
        seg_output = np.where(seg_output == 3, 4, seg_output).astype(np.uint8)
        prediction_ni = nib.Nifti1Image(seg_output, brain_affine)
        prediction_ni.to_filename(os.path.join(save_path, '{}.nii.gz'.format(patient_id)))
    else:
        print(f"Skipping non-file: {file_path}")

model.summary()

!pip install graphviz

from graphviz import Digraph

# Diyagram oluşturma
dot = Digraph(comment='Deep Learning Workflow for Brain Tumor Segmentation')

# Genel akışın ana adımları
dot.node('A', 'Başlangıç', shape='ellipse', color='lightblue')
dot.node('B', 'Veri Yükleme ve Ön İşleme', shape='box', color='lightgreen')
dot.node('C', 'Veri Bölme', shape='box', color='lightgreen')
dot.node('D', 'Model Oluşturma', shape='box', color='lightyellow')
dot.node('E', 'Model Eğitim', shape='box', color='lightcoral')
dot.node('F', 'Model Değerlendirme', shape='box', color='lightpink')
dot.node('G', 'Tahmin Yapma', shape='box', color='lightgrey')
dot.node('H', 'Sonuçların Görselleştirilmesi', shape='box', color='lightgoldenrod')
dot.node('I', 'Sonuçların Analizi ve Raporlama', shape='box', color='lightblue')
dot.node('J', 'Bitiş', shape='ellipse', color='lightblue')

# İlişkiler
dot.edge('A', 'B', label='Başlangıç')
dot.edge('B', 'C', label='Veri Yükleme')
dot.edge('C', 'D', label='Veri Bölme')
dot.edge('D', 'E', label='Model Oluşturma')
dot.edge('E', 'F', label='Model Eğitim')
dot.edge('F', 'G', label='Model Değerlendirme')
dot.edge('G', 'H', label='Tahmin Yapma')
dot.edge('H', 'I', label='Sonuçların Görselleştirilmesi')
dot.edge('I', 'J', label='Sonuçların Analizi ve Raporlama')

# Gelişmiş detaylar
dot.node('D1', 'SGANet ve UNet Model Seçimi', shape='subroutine', color='lightgrey')
dot.node('D2', 'Hyperparameter Tuning', shape='subroutine', color='lightgrey')
dot.node('D3', 'Model Mimarisinin Tanımlanması', shape='subroutine', color='lightgrey')

dot.edge('D', 'D1', label='Model Seçimi')
dot.edge('D1', 'D2', label='Hyperparameter Tuning')
dot.edge('D2', 'D3', label='Model Mimarisinin Tanımlanması')

dot.node('E1', 'Loss ve Accuracy İzleme', shape='subroutine', color='lightgrey')
dot.node('E2', 'Optimizer ve Learning Rate Ayarları', shape='subroutine', color='lightgrey')

dot.edge('E', 'E1', label='Eğitim Süreci')
dot.edge('E1', 'E2', label='Optimizer Ayarları')

dot.node('F1', 'K-Fold Cross Validation', shape='subroutine', color='lightgrey')
dot.node('F2', 'Performans Metriği Hesaplama', shape='subroutine', color='lightgrey')

dot.edge('F', 'F1', label='Değerlendirme Yöntemi')
dot.edge('F1', 'F2', label='Metriğin Hesaplanması')

dot.node('G1', 'Tahminlerin Doğruluğunu Kontrol Etme', shape='subroutine', color='lightgrey')

dot.edge('G', 'G1', label='Tahmin Kontrolü')

dot.node('H1', 'Görsel Analiz ve Raporlama', shape='subroutine', color='lightgrey')

dot.edge('H', 'H1', label='Görselleştirme')

# Diyagramı kaydetme
dot.render('deep_learning_workflow', format='png', cleanup=True)

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, UpSampling2D
from tensorflow.keras.utils import plot_model
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Modeli tanımla
input_layer = Input(shape=(200, 200, 4))

x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_layer)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
x = MaxPooling2D((2, 2))(x)

x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)
x = UpSampling2D((2, 2))(x)

x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)
x = UpSampling2D((2, 2))(x)

x = Conv2DTranspose(16, (3, 3), activation='relu', padding='same')(x)
x = UpSampling2D((2, 2))(x)

output_layer = Conv2D(4, (3, 3), activation='softmax', padding='same')(x)

model = Model(inputs=input_layer, outputs=output_layer)

# Model yapısını çiz
plot_model(model, to_file='/content/model_plot.png', show_shapes=True, show_layer_names=True)

# Resmi göster
img = mpimg.imread('/content/model_plot.png')
plt.figure(figsize=(10, 10))
plt.imshow(img)
plt.axis('off')  # Axisleri kapalı tut
plt.show()